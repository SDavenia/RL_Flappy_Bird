{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import text_flappy_bird_gym\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"TextFlappyBird-v0\", height=15, width=20, pipe_gap=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expected_SARSA:\n",
    "    def __init__(self, space_size, action_size, gamma=1):\n",
    "        \"\"\"\n",
    "        Calculates optimal policy Expected SARSA.\n",
    "        This code does NOT support TD lambda but only TD(0)\n",
    "        \"\"\"\n",
    "        # the discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # size of system\n",
    "        self.space_size = space_size  # as tuple\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # where to save returns\n",
    "        self.Qvalues = np.zeros((*self.space_size, self.action_size))\n",
    "\n",
    "    def get_action_epsilon_greedy(self, s):\n",
    "        \"\"\"\n",
    "        Chooses action at random using an epsilon-greedy policy wrt the current Q(s,a).\n",
    "        \"\"\"\n",
    "        ran = np.random.rand()\n",
    "\n",
    "        if ran < self.epsilon:\n",
    "            prob_actions = np.ones(self.action_size) / self.action_size\n",
    "\n",
    "        else:\n",
    "            best_value = np.max(self.Qvalues[(*s,)])\n",
    "\n",
    "            best_actions = self.Qvalues[(*s,)] == best_value\n",
    "\n",
    "            prob_actions = best_actions / np.sum(best_actions)\n",
    "\n",
    "        a = np.random.choice(self.action_size, p=prob_actions)\n",
    "        return a\n",
    "\n",
    "    def policy(self, s, eps):\n",
    "        \"\"\"\n",
    "        Probabilities from an epsilon-greedy policy wrt the current Q(s,a).\n",
    "        \"\"\"\n",
    "        policy = np.ones(self.action_size) / self.action_size * eps\n",
    "\n",
    "        best_value = np.max(self.Qvalues[(*s,)])\n",
    "       \n",
    "        best_actions = self.Qvalues[(*s,)] == best_value\n",
    "        policy += best_actions / np.sum(best_actions) * (1 - eps)\n",
    "        return policy\n",
    "\n",
    "    def single_step_update(self, s, a, r, new_s, new_a, done):\n",
    "        \"\"\"\n",
    "        Uses a single step to update the values, using Temporal Difference for Q values.\n",
    "        Employs the EXPERIENCED action in the new state  <- Q(S_new, A_new).\n",
    "        \"\"\"\n",
    "        self.et[(*s, a)] += 1\n",
    "\n",
    "        # If we reached the terminal state\n",
    "        if done:\n",
    "            deltaQ = r + 0 - self.Qvalues[(*s, a)]\n",
    "            self.Qvalues += self.lr_v * deltaQ * self.et\n",
    "\n",
    "        else:\n",
    "            deltaQ = (\n",
    "                r\n",
    "                + self.gamma * np.dot(self.Qvalues[(*new_s,)], self.policy(new_s, eps))\n",
    "                - self.Qvalues[(*s, a)]\n",
    "            )\n",
    "        self.Qvalues[(*s, a)] += self.lr_v * deltaQ\n",
    "    \n",
    "\n",
    "\n",
    "    def train(self, n_episodes=10000, tstar=None, epsilon_0=0.2,k_epsilon=0, lr_v0=0.15, k_lr=0):\n",
    "        \"\"\"\n",
    "        This function trains the agent using n_episodes.\n",
    "        The default parameters use constant learning rate and epsilon (k = 0 in both cases)\n",
    "        Otherwise a decaying rate is implemented after a starting point t0 (see README for more details)\n",
    "        \"\"\"\n",
    "\n",
    "        Actions = [i for i in range(self.action_size)]\n",
    "        self.n_episodes = n_episodes\n",
    "\n",
    "        # Add the following attributes to the class\n",
    "        self.performance_traj = np.zeros(n_episodes)  # To store cumulative reward at every game\n",
    "\n",
    "        # Parameters for epsilon decay\n",
    "        self.epsilon_0 = epsilon_0  # Needed to name the plots\n",
    "        self.epsilon = epsilon_0  # Needed to keep track of current epsilon\n",
    "        self.k_epsilon = k_epsilon\n",
    "\n",
    "        # Parameters for learning rate decay\n",
    "        self.lr_v0 = lr_v0\n",
    "        self.lr_v = lr_v0\n",
    "        self.k_lr = k_lr\n",
    "\n",
    "        if tstar is None:\n",
    "            tstar = 2.5 * n_episodes\n",
    "\n",
    "        count = 0  # counter variable needed to see when to start decaying rates\n",
    "\n",
    "        # Run over episodes\n",
    "        for i in range(n_episodes):\n",
    "            done = False\n",
    "            s, info = env.reset()\n",
    "            a = self.get_action_epsilon_greedy(s)\n",
    "            act = Actions[a]\n",
    "\n",
    "            while not done:\n",
    "                count += 1\n",
    "\n",
    "                # Perform one \"step\" in the environment\n",
    "                new_s, r, done, _, info = env.step(act)\n",
    "\n",
    "                # Keep track of rewards for one episode\n",
    "                self.performance_traj[i] += r\n",
    "\n",
    "                # Choose new action index\n",
    "                new_a = self.get_action_epsilon_greedy(new_s)\n",
    "                act = Actions[new_a]\n",
    "\n",
    "                self.single_step_update(s, a, r, new_s, new_a, done)\n",
    "\n",
    "                if count > tstar:\n",
    "                    self.epsilon = epsilon_0 / (1.0 + self.k_epsilon * (count - tstar) ** 1.05)\n",
    "                    self.lr_v = lr_v0 / (1 + self.k_lr * (count - tstar) ** 0.75)\n",
    "\n",
    "                a = new_a\n",
    "                s = new_s\n",
    "\n",
    "    def plot_traj(self, cumulative=True, local=False, save_img=False, save_traj=False):\n",
    "        title = \"SARSA\"\n",
    "\n",
    "        plot_indexes = np.arange(0, self.n_episodes + 1, 20, dtype=int)\n",
    "        plot_indexes[-1] = plot_indexes[-1] - 1\n",
    "\n",
    "        plt.plot(plot_indexes, self.performance_traj[plot_indexes])\n",
    "\n",
    "        if cumulative:\n",
    "            cumulative_mean = np.cumsum(self.performance_traj) / np.arange(1, len(self.performance_traj) + 1)\n",
    "            plt.plot(plot_indexes, cumulative_mean[plot_indexes], label=\"Cumulative mean\")\n",
    "        if local:\n",
    "            window_size = 100\n",
    "            local_mean_SARSA = np.convolve(self.performance_traj, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "            plt.plot(plot_indexes[plot_indexes < local_mean_SARSA.shape[0]],\n",
    "                     local_mean_SARSA[plot_indexes[plot_indexes < local_mean_SARSA.shape[0]]],\n",
    "                     label=\" Local Mean\", color = 'red')\n",
    "            \n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Episode reward\")\n",
    "        plt.legend()\n",
    "        plt.suptitle(f\"{title} control cumulative rewards\")\n",
    "\n",
    "        plt.title(f\"$\\epsilon_0$ = {self.epsilon_0}, $k_\\epsilon$ = {self.k_epsilon}, $\\\\alpha_0$ = {self.lr_v0}, $k_{{\\\\alpha}}$ = {self.k_lr}\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_img:\n",
    "            name = (\n",
    "                \"Plots/Exp_SARSA_plots/Exp_SARSA_k_alpha_\"\n",
    "                + str(self.k_lr)\n",
    "                + \"k_epsilon\"\n",
    "                + str(self.k_epsilon)\n",
    "                + \".png\"\n",
    "            )\n",
    "            plt.savefig(name)\n",
    "        plt.show()\n",
    "\n",
    "        if save_traj:\n",
    "            name = (\n",
    "                \"Trajectories/Exp_SARSA_trajectories/Exp_SARSA_k_alpha_\"\n",
    "                + str(self.k_lr)\n",
    "                + \"k_epsilon\"\n",
    "                + str(self.k_epsilon)\n",
    "                + \".txt\"\n",
    "            )\n",
    "            array_str = \" \".join(str(element) for element in self.performance_traj)\n",
    "            file = open(name, 'w+')\n",
    "            file.write(array_str)\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = (env.observation_space[0].n, env.observation_space[1].n)\n",
    "n_episodes = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.0, 0.2, 0.5, 0.8]\n",
    "k_epsilons = [0.00005, 0.0005, 0.005, 0.05]\n",
    "k_lrs = [0.00003, 0.0003, 0.003, 0.03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lambdas[0]:\n",
    "    for j in k_epsilons[0]:\n",
    "        for l in k_lrs[0]:\n",
    "            e_sarsa = Expected_SARSA(space_size=observation_space, action_size=2, gamma=1)\n",
    "            e_sarsa.train(n_episodes=n_episodes, lambda_=i, k_epsilon=j, k_lr=l)\n",
    "            e_sarsa.plot_traj(cumulative=True, local=True, save_img=True, save_traj=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lambdas:\n",
    "   \"Last run to include case with no decaying\"\n",
    "   k_e = 0\n",
    "   k_lr = 0\n",
    "   e_sarsa = Expected_SARSA(space_size=observation_space, action_size=2, gamma=1)\n",
    "   e_sarsa.train(n_episodes=n_episodes, lambda_=i, k_epsilon=k_e, k_lr=k_lr)\n",
    "   e_sarsa.plot_traj(cumulative=True, local=True, save_img=True, save_traj=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappy_bird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
