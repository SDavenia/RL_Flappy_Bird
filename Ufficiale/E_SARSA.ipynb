{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import text_flappy_bird_gym\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"TextFlappyBird-v0\", height=15, width=20, pipe_gap=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Expected SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    def __init__(self, space_size, action_size, gamma=1):\n",
    "        \"\"\"\n",
    "        Calculates optimal policy using in-policy Temporal Difference control\n",
    "        Evaluates Q-value for (S,A) pairs, using one-step updates.\n",
    "        \"\"\"\n",
    "        # the discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # size of system\n",
    "        self.space_size = space_size  # as tuple\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # where to save returns\n",
    "        self.Qvalues = np.zeros((*self.space_size, self.action_size))\n",
    "\n",
    "    def get_action_epsilon_greedy(self, s):\n",
    "        \"\"\"\n",
    "        Chooses action at random using an epsilon-greedy policy wrt the current Q(s,a).\n",
    "        \"\"\"\n",
    "        ran = np.random.rand()\n",
    "\n",
    "        if ran < self.epsilon:\n",
    "            prob_actions = np.ones(self.action_size) / self.action_size\n",
    "\n",
    "        else:\n",
    "            best_value = np.max(self.Qvalues[(*s,)])\n",
    "\n",
    "            best_actions = self.Qvalues[(*s,)] == best_value\n",
    "\n",
    "            prob_actions = best_actions / np.sum(best_actions)\n",
    "\n",
    "        a = np.random.choice(self.action_size, p=prob_actions)\n",
    "        return a\n",
    "\n",
    "    def single_step_update(self, s, a, r, new_s, new_a, done):\n",
    "        \"\"\"\n",
    "        Uses a single step to update the values, using Temporal Difference for Q values.\n",
    "        Employs the EXPERIENCED action in the new state  <- Q(S_new, A_new).\n",
    "        \"\"\"\n",
    "        self.et[(*s, a)] += 1\n",
    "\n",
    "        # If we reached the terminal state\n",
    "        if done:\n",
    "            deltaQ = r + 0 - self.Qvalues[(*s, a)]\n",
    "            self.Qvalues += self.lr_v * deltaQ * self.et\n",
    "\n",
    "        else:\n",
    "            deltaQ = (\n",
    "                r + self.gamma * self.Qvalues[(*new_s, new_a)] - self.Qvalues[(*s, a)]\n",
    "            )\n",
    "\n",
    "            self.Qvalues += self.lr_v * deltaQ * self.et\n",
    "            self.et *= self.gamma * self.lambda_\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        n_episodes=10000,\n",
    "        lambda_=0,\n",
    "        tstar=None,\n",
    "        epsilon_0=0.2,\n",
    "        k_epsilon=0,\n",
    "        lr_v0=0.15,\n",
    "        k_lr=0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This function trains the agent using n_episodes.\n",
    "        The default parameters use constant learning rate and epsilon (k = 0 in both cases)\n",
    "        Otherwise a decaying rate is implemented after a starting point t0 (see README for more details)\n",
    "        Similarly the default implements a TD(0) evaluation procedure (lambda = 0)\n",
    "        \"\"\"\n",
    "\n",
    "        Actions = [i for i in range(self.action_size)]\n",
    "        self.n_episodes = n_episodes\n",
    "\n",
    "        # Add the following attributes to the class\n",
    "        self.performance_traj = np.zeros(n_episodes)  # To store cumulative reward at every game\n",
    "\n",
    "        self.et = np.zeros((*self.space_size, self.action_size))\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        # Parameters for epsilon decay\n",
    "        self.epsilon_0 = epsilon_0  # Needed to name the plots\n",
    "        self.epsilon = epsilon_0  # Needed to keep track of current epsilon\n",
    "        self.k_epsilon = k_epsilon\n",
    "\n",
    "        # Parameters for learning rate decay\n",
    "        self.lr_v0 = lr_v0\n",
    "        self.lr_v = lr_v0\n",
    "        self.k_lr = k_lr\n",
    "\n",
    "        if tstar is None:\n",
    "            tstar = 2.5 * n_episodes\n",
    "\n",
    "        count = 0  # counter variable needed to see when to start decaying rates\n",
    "\n",
    "        # Run over episodes\n",
    "        for i in range(n_episodes):\n",
    "            done = False\n",
    "            s, info = env.reset()\n",
    "            a = self.get_action_epsilon_greedy(s)\n",
    "            act = Actions[a]\n",
    "\n",
    "            while not done:\n",
    "                count += 1\n",
    "\n",
    "                # Perform one \"step\" in the environment\n",
    "                new_s, r, done, _, info = env.step(act)\n",
    "\n",
    "                # Keep track of rewards for one episode\n",
    "                self.performance_traj[i] += r\n",
    "\n",
    "                # Choose new action index\n",
    "                new_a = self.get_action_epsilon_greedy(new_s)\n",
    "                act = Actions[new_a]\n",
    "                # Single update with (S, A, R', S', A')\n",
    "                self.single_step_update(s, a, r, new_s, new_a, done)\n",
    "\n",
    "                if count > tstar:\n",
    "                    self.epsilon = epsilon_0 / (1.0 + self.k_epsilon * (count - tstar) ** 1.05)\n",
    "                    self.lr_v = lr_v0 / (1 + self.k_lr * (count - tstar) ** 0.75)\n",
    "\n",
    "                a = new_a\n",
    "                s = new_s\n",
    "\n",
    "    def plot_traj(self, cumulative=True, local=False, save=False):\n",
    "        title = \"SARSA\"\n",
    "\n",
    "        plot_indexes = np.arange(0, self.n_episodes + 1, 20, dtype=int)\n",
    "        plot_indexes[-1] = plot_indexes[-1] - 1\n",
    "\n",
    "        plt.plot(plot_indexes, self.performance_traj[plot_indexes])\n",
    "\n",
    "        if cumulative:\n",
    "            cumulative_mean = np.cumsum(self.performance_traj) / np.arange(1, len(self.performance_traj) + 1)\n",
    "            plt.plot(plot_indexes, cumulative_mean[plot_indexes], label=\"Cumulative mean\")\n",
    "        if local:\n",
    "            window_size = 100\n",
    "            local_mean_SARSA = np.convolve(self.performance_traj, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "            plt.plot(plot_indexes[plot_indexes < local_mean_SARSA.shape[0]],\n",
    "                     local_mean_SARSA[plot_indexes[plot_indexes < local_mean_SARSA.shape[0]]],\n",
    "                     label=\" Local Mean\", color = 'red')\n",
    "            \n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Episode reward\")\n",
    "        plt.legend()\n",
    "        plt.suptitle(f\"{title} control cumulative rewards\")\n",
    "\n",
    "        plt.title(f\"$\\epsilon_0$ = {self.epsilon_0}, $k_\\epsilon$ = {self.k_epsilon}, $\\\\alpha_0$ = {self.lr_v0}, $k_{{\\\\alpha}}$ = {self.k_lr}, $\\lambda$ = {self.lambda_}\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            name = (\n",
    "                \"Plots/SARSA_plots/SARSA_lambda_\" + str(self.lambda_)\n",
    "                + \"_k_alpha_\"\n",
    "                + str(self.k_lr)\n",
    "                + \"k_epsilon\"\n",
    "                + str(self.k_epsilon)\n",
    "                + \".png\"\n",
    "            )\n",
    "            plt.savefig(name)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappy_bird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
