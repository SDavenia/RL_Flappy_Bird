{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import text_flappy_bird_gym\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"TextFlappyBird-v0\", height=15, width=20, pipe_gap=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement first-visit on-policy MC Control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class MC_Control:\n",
    "    def __init__(self, space_size, action_size, gamma=1):\n",
    "        # -------\n",
    "        self.gamma = gamma\n",
    "        self.space_size = space_size\n",
    "        self.action_size = action_size\n",
    "        self.Qvalues = np.zeros((*self.space_size, self.action_size))\n",
    "\n",
    "        # ------- serve solo a MC\n",
    "        self.returns = [\n",
    "            [[[] for _ in range(2)] for _ in range(self.space_size[0])]\n",
    "            for _ in range(self.space_size[1])\n",
    "        ]\n",
    "\n",
    "    def discount_cumsum(self, x, discount):\n",
    "        return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "    def get_action_epsilon_greedy(self, s, eps):\n",
    "        \"\"\"\n",
    "        Chooses action at random using an epsilon-greedy policy wrt the current Q(s,a).\n",
    "        \"\"\"\n",
    "        ran = np.random.rand()\n",
    "\n",
    "        if ran < eps:\n",
    "            prob_actions = np.ones(self.action_size) / self.action_size\n",
    "\n",
    "        else:\n",
    "            best_value = np.max(self.Qvalues[(*s,)])\n",
    "\n",
    "            best_actions = self.Qvalues[(*s,)] == best_value\n",
    "\n",
    "            prob_actions = best_actions / np.sum(best_actions)\n",
    "\n",
    "        a = np.random.choice(self.action_size, p=prob_actions)\n",
    "        return a\n",
    "\n",
    "    def greedy_policy(self):\n",
    "        a = np.argmax(self.Qvalues, axis=2)\n",
    "        return a\n",
    "\n",
    "    # -------- serve solo a MC\n",
    "    def single_episode_update(self, traj_states, traj_rew, traj_act):\n",
    "        \"\"\"\n",
    "        Uses a single trajectory to update the Qvalues, using first-visit MC.\n",
    "        \"\"\"\n",
    "        # keep track of visited pair (state, action)\n",
    "        visited_pairs = []\n",
    "        remaining_pairs = [(traj_states[t], traj_act[t]) for t in range(len(traj_act))]\n",
    "\n",
    "        # calculates the returns for each step: DISCOUNTed CUMulative SUM.\n",
    "        ret = self.discount_cumsum(traj_rew, self.gamma)\n",
    "\n",
    "        # given teh current episode, take the last pair (St, At) and go backward\n",
    "        for t_step, s in reversed(list(enumerate(traj_states))):\n",
    "            # get the action taken after being in state s\n",
    "            action = traj_act[t_step]\n",
    "\n",
    "            # build the pair (St, At) = ((x,y), At)\n",
    "            pair = (s, action)\n",
    "\n",
    "            remaining_pairs.pop()\n",
    "\n",
    "            if pair in remaining_pairs:\n",
    "                # print(\"pair: \", pair)\n",
    "                s = pair[0]\n",
    "                action = pair[1]\n",
    "                # print(s)\n",
    "\n",
    "                self.returns[s[0]][s[1]][action].append(ret[t_step])\n",
    "            else:\n",
    "                self.returns[s[0]][s[1]][action].append(ret[t_step])\n",
    "                visited_pairs.append((s, action))\n",
    "\n",
    "        for pair in visited_pairs:\n",
    "            s = pair[0]\n",
    "            action = pair[1]\n",
    "            self.Qvalues[s[0]][s[1]][action] = np.mean(self.returns[s[0]][s[1]][action])\n",
    "\n",
    "    def train(self, n_episodes=10000, tstar=None, epsilon_0=0.2, k=0.0):\n",
    "        # new attributes\n",
    "        self.performance_traj = np.zeros(n_episodes)\n",
    "        self.epsilon_0 = epsilon_0\n",
    "        self.k = k\n",
    "        self.n_episodes = n_episodes\n",
    "\n",
    "        count = 0\n",
    "        if tstar is None:\n",
    "            tstar = 2.5 * n_episodes\n",
    "        epsilon = epsilon_0\n",
    "        Actions = [i for i in range(self.action_size)]\n",
    "\n",
    "        for i in range(n_episodes):\n",
    "            traj_states = []\n",
    "            traj_rew = []\n",
    "            traj_act = []\n",
    "            done = False\n",
    "\n",
    "            s, info = env.reset()\n",
    "            a = self.get_action_epsilon_greedy(s=s, eps=epsilon)\n",
    "            act = Actions[a]\n",
    "\n",
    "            while not done:\n",
    "                count += 1\n",
    "\n",
    "                new_s, r, done, _, info = env.step(act)\n",
    "                traj_states.append(s)\n",
    "                traj_act.append(a)\n",
    "                traj_rew.append(r)\n",
    "\n",
    "                # Keeps track of performance for each episode\n",
    "                self.performance_traj[i] += r\n",
    "\n",
    "                # Choose new action index\n",
    "                new_a = self.get_action_epsilon_greedy(new_s, epsilon)\n",
    "\n",
    "                # (Corresponding action to index)\n",
    "                act = Actions[new_a]\n",
    "                a = new_a\n",
    "                s = new_s\n",
    "\n",
    "                if count > tstar:\n",
    "                    # UPDATE OF EPSILON\n",
    "                    epsilon = epsilon_0 / (1.0 + k * (count - tstar) ** 1.05)\n",
    "\n",
    "            # MC step at the end of the episode (averaging)\n",
    "            self.single_episode_update(traj_states, traj_rew, traj_act)\n",
    "\n",
    "    def plot_traj(self, cumulative=True, local=False, save=False):\n",
    "        # CAMBIARE IL TITOLO\n",
    "        title = \"MC\"\n",
    "\n",
    "        plot_indexes = np.arange(0, self.n_episodes + 1, 20, dtype=int)\n",
    "        plot_indexes[-1] = plot_indexes[-1] - 1\n",
    "\n",
    "        plt.plot(plot_indexes, self.performance_traj[plot_indexes])\n",
    "\n",
    "        if cumulative:\n",
    "            cumulative_mean = np.cumsum(self.performance_traj) / np.arange(\n",
    "                1, len(self.performance_traj) + 1\n",
    "            )\n",
    "            plt.plot(\n",
    "                plot_indexes, cumulative_mean[plot_indexes], label=\"Cumulative mean\"\n",
    "            )\n",
    "        if local:\n",
    "            window_size = 100\n",
    "            local_mean_SARSA = np.convolve(\n",
    "                self.performance_traj, np.ones(window_size) / window_size, mode=\"valid\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                plot_indexes[plot_indexes < local_mean_SARSA.shape[0]],\n",
    "                local_mean_SARSA[\n",
    "                    plot_indexes[plot_indexes < local_mean_SARSA.shape[0]]\n",
    "                ],\n",
    "                label=\" Local Mean\",\n",
    "            )\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Episode reward\")\n",
    "        plt.legend()\n",
    "        plt.suptitle(f\"{title} control cumulative rewards\")\n",
    "        plt.title(f\"Epsilon0 = {self.epsilon_0}, k = {self.k}\")\n",
    "\n",
    "        # SISTEMARE SALVATAGGIO IN CARTELLA GIUSTA\n",
    "        if save:\n",
    "            name = \"Plots/MC_plots/MC_k_epsilon\" + str(self.k_epsilon) + \".png\"\n",
    "            plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = (env.observation_space[0].n, env.observation_space[1].n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = MC_Control(space_size=observation_space, action_size=2, gamma=1)\n",
    "MC.train(n_episodes=10000, epsilon_0=0.2, k=0.0)\n",
    "MC.plot_traj(cumulative=True, local=True, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = MC_Control(space_size=observation_space, action_size=2, gamma=1)\n",
    "MC.train(n_episodes=10000, epsilon_0=0.2, k=0.00005)\n",
    "MC.plot_traj(cumulative=True, local=True, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = MC_Control(space_size=observation_space, action_size=2, gamma=1)\n",
    "MC.train(n_episodes=10000, epsilon_0=0.2, k=0.0005)\n",
    "MC.plot_traj(cumulative=True, local=True, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = MC_Control(space_size=observation_space, action_size=2, gamma=1)\n",
    "MC.train(n_episodes=10000, epsilon_0=0.2, k=0.005)\n",
    "MC.plot_traj(cumulative=True, local=True, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = MC_Control(space_size=observation_space, action_size=2, gamma=1)\n",
    "MC.train(n_episodes=10000, epsilon_0=0.2, k=0.05)\n",
    "MC.plot_traj(cumulative=True, local=True, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappy_bird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
