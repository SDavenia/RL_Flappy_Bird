{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import text_flappy_bird_gym\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"TextFlappyBird-v0\", height=15, width=20, pipe_gap=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Q method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Learning:\n",
    "    def __init__(self, space_size, action_size, gamma=1):\n",
    "        \"\"\"\n",
    "        Calculates optimal policy using in-policy Temporal Difference control\n",
    "        Evaluates Q-value for (S,A) pairs, using one-step updates.\n",
    "        \"\"\"\n",
    "        # the discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # size of system\n",
    "        self.space_size = space_size  # as tuple\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # where to save returns\n",
    "        self.Qvalues = np.zeros((*self.space_size, self.action_size))\n",
    "    \n",
    "    def get_action_epsilon_greedy(self, s):\n",
    "        \"\"\"\n",
    "        Chooses action at random using an epsilon-greedy policy wrt the current Q(s,a).\n",
    "        \"\"\"\n",
    "        ran = np.random.rand()\n",
    "\n",
    "        if ran < self.epsilon:\n",
    "            prob_actions = np.ones(self.action_size) / self.action_size\n",
    "\n",
    "        else:\n",
    "            best_value = np.max(self.Qvalues[(*s,)])\n",
    "\n",
    "            best_actions = self.Qvalues[(*s,)] == best_value\n",
    "\n",
    "            prob_actions = best_actions / np.sum(best_actions)\n",
    "\n",
    "        a = np.random.choice(self.action_size, p=prob_actions)\n",
    "        return a\n",
    "    \n",
    "    def single_step_update(self, s, a, r, new_s, done):\n",
    "        \"\"\"\n",
    "        Uses a single step to update the values, using Temporal Difference for Q values.\n",
    "        Unlike sarsa, the new action new_a is not used, as it is chosen as the argmax of Q wrt s_new.\n",
    "        In this case it is stored as it is needed for the implementation of Q(lambda)\n",
    "        \"\"\"\n",
    "        if self.old_action is not None and a != self.old_action:\n",
    "            self.et = np.zeros((*self.space_size, self.action_size))\n",
    "        else:\n",
    "            self.et *= self.gamma*self.lambda_  \n",
    "\n",
    "        self.et[(*s, a)] += 1\n",
    "\n",
    "        if done:\n",
    "            deltaQ = r + 0 - self.Qvalues[(*s, a)]\n",
    "            self.Qvalues += self.lr_v*deltaQ*self.et\n",
    "\n",
    "        else:\n",
    "            maxQ_over_actions = np.max(self.Qvalues[(*new_s,)])\n",
    "            a_star = np.argmax(self.Qvalues[(*new_s,)])\n",
    "            self.old_action = a_star\n",
    "\n",
    "            deltaQ = r + self.gamma * maxQ_over_actions - self.Qvalues[(*s, a)]\n",
    "\n",
    "            self.Qvalues += self.lr_v*deltaQ*self.et\n",
    "    \n",
    "    def train(self, n_episodes = 10000, lambda_ = 0, tstar = None, epsilon_0=0.2, k_epsilon = 0, lr_v0=0.15, k_lr = 0):\n",
    "        \"\"\"\n",
    "        This function trains the agent using n_episodes.\n",
    "        The default parameters use constant learning rate and epsilon (k = 0 in both cases)\n",
    "        Otherwise a decaying rate is implemented after a starting point t0 (see README for more details)\n",
    "        Similarly the default implements a TD(0) evaluation procedure (lambda = 0)\n",
    "        \"\"\"\n",
    "\n",
    "        Actions = [i for i in range(self.action_size)]\n",
    "        self.n_episodes = n_episodes\n",
    "\n",
    "        # Add the following attributes to the class\n",
    "        self.performance_traj = np.zeros(n_episodes) # To store cumulative reward at every game\n",
    "        \n",
    "        self.et = np.zeros((*self.space_size, self.action_size))\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        # Parameters for epsilon decay\n",
    "        self.epsilon_0 = epsilon_0  # Needed to name the plots\n",
    "        self.epsilon = epsilon_0    # Needed to keep track of current epsilon\n",
    "        self.k_epsilon = k_epsilon\n",
    "\n",
    "        # Parameters for learning rate decay\n",
    "        self.lr_v0 = lr_v0\n",
    "        self.lr_v = lr_v0\n",
    "        self.k_lr = k_lr\n",
    "\n",
    "        self.old_action = None # Needed for Q(lamba) learning\n",
    "\n",
    "        if(tstar is None):\n",
    "            tstar = 2.5 * n_episodes \n",
    "        \n",
    "        count = 0 # counter variable needed to see when to start decaying rates\n",
    "\n",
    "        # Run over episodes\n",
    "        for i in range(n_episodes):\n",
    "            done = False\n",
    "            s, info = env.reset()\n",
    "            a = self.get_action_epsilon_greedy(s)\n",
    "            act = Actions[a]\n",
    "\n",
    "            while not done:\n",
    "                count += 1\n",
    "\n",
    "                # Perform one \"step\" in the environment\n",
    "                new_s, r, done, _, info = env.step(act)\n",
    "\n",
    "                # Keep track of rewards for one episode\n",
    "                self.performance_traj[i] += r\n",
    "\n",
    "                # Determine what action the agent would choose according to optimal policy\n",
    "                new_a = self.get_action_epsilon_greedy(new_s)\n",
    "                act = Actions[new_a]\n",
    "                # Single update with (S, A, R', S')\n",
    "                self.single_step_update(s, a, r, new_s, done)\n",
    "\n",
    "                if count > tstar:\n",
    "                    self.epsilon = epsilon_0/(1. + self.k_epsilon*(count - tstar)**1.05)\n",
    "                    self.lr_v = lr_v0/(1 + self.k_lr*(count - tstar)**0.75)\n",
    "                \n",
    "                a = new_a\n",
    "                s = new_s\n",
    "    \n",
    "    def plot_traj(self, cumulative=True, local=False, save = False):\n",
    "\n",
    "        title = \"Q Learning\"\n",
    "\n",
    "        plot_indexes = np.arange(0, self.n_episodes + 1, 20, dtype=int)\n",
    "        plot_indexes[-1] = plot_indexes[-1] - 1\n",
    "\n",
    "        plt.plot(plot_indexes, self.performance_traj[plot_indexes])\n",
    "\n",
    "        if cumulative:\n",
    "            cumulative_mean = np.cumsum(self.performance_traj) / np.arange(1, len(self.performance_traj) + 1)\n",
    "            plt.plot(plot_indexes, cumulative_mean[plot_indexes], label=\"Cumulative mean\")\n",
    "        \n",
    "        if local:\n",
    "            window_size = 100\n",
    "            local_mean_SARSA = np.convolve(self.performance_traj, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "            plt.plot(plot_indexes[plot_indexes < local_mean_SARSA.shape[0]],\n",
    "                     local_mean_SARSA[plot_indexes[plot_indexes < local_mean_SARSA.shape[0]]],\n",
    "                     label=\" Local Mean\", color = 'red')\n",
    "            \n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Episode reward\")\n",
    "        plt.legend()\n",
    "        plt.suptitle(f\"{title} control cumulative rewards\")\n",
    "\n",
    "        plt.title(f\"$\\epsilon_0$ = {self.epsilon_0}, $k_\\epsilon$ = {self.k_epsilon}, $\\\\alpha_0$ = {self.lr_v0}, $k_{{\\\\alpha}}$ = {self.k_lr}, $\\lambda$ = {self.lambda_}\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save:\n",
    "            name = (\n",
    "                \"Plots/Q_plots/Q_lambda\" + str(self.lambda_)\n",
    "                + \"_k_alpha_\"\n",
    "                + str(self.k_lr)\n",
    "                + \"k_epsilon\"\n",
    "                + str(self.k_epsilon)\n",
    "                + \".png\"\n",
    "            )\n",
    "            plt.savefig(name)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = (env.observation_space[0].n, env.observation_space[1].n)\n",
    "n_episodes = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.0, 0.2, 0.5, 0.8]\n",
    "k_epsilons = [0.00005, 0.0005, 0.005, 0.05]\n",
    "k_lrs = [0.00003, 0.0003, 0.003, 0.03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lambdas:\n",
    "    for j in k_epsilons:\n",
    "        for l in k_lrs:\n",
    "            q_learning = Q_Learning(space_size=observation_space, action_size=2, gamma=1)\n",
    "            q_learning.train(n_episodes=n_episodes, lambda_=i, k_epsilon=j, k_lr=l)\n",
    "            q_learning.plot_traj(cumulative=True, local=True, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flappy_bird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
