{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing On-policy MC control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import flappy_bird_gymnasium"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC control\n",
    "\n",
    "https://github.com/sudharsan13296/Deep-Reinforcement-Learning-With-Python/blob/master/04.%20Monte%20Carlo%20Methods/4.13.%20Implementing%20On-Policy%20MC%20Control.ipynb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using flappy_bird_gymnasium\n",
    "\n",
    "https://github.com/markub3327/flappy-bird-gymnasium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "while True:\n",
    "    # Next action:\n",
    "    # (feed the observation to your agent here)\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Processing:\n",
    "    obs, reward, terminated, _, info = env.step(action)\n",
    "\n",
    "    # Rendering the game:\n",
    "    # (remove this two lines during training)\n",
    "    env.render()\n",
    "    time.sleep(1 / 30)  # FPS\n",
    "\n",
    "    # Checking if the player is still alive\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dictionary for storing the Q values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = defaultdict(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dictionary for storing the total return of the state-action pair:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return = defaultdict(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dictionary for storing the count of the number of times a state-action pair is visited:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = defaultdict(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the epsilon-greedy policy\n",
    "\n",
    "We learned that we select actions based on the epsilon-greedy policy, so we define a function called epsilon_greedy_policy which takes the state and Q value as an input and returns the action to be performed in the given state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q):\n",
    "    # set the epsilon value to 0.5\n",
    "    epsilon = 0.5\n",
    "\n",
    "    # sample a random value from the uniform distribution, if the sampled value is less than\n",
    "    # epsilon then we select a random action else we select the best action which has maximum Q\n",
    "    # value as shown below\n",
    "\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(\n",
    "            list(range(env.action_space.n)), key=lambda x: Q[(tuple(state[0]), x)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy_(state, Q):\n",
    "    epsilon = 0.5\n",
    "\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        state_tuple = tuple(state.tolist())  # Convert the NumPy array to a tuple\n",
    "        return max(list(range(env.action_space.n)), key=lambda x: Q[(state_tuple, x)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating an epidose\n",
    "\n",
    "Now, let's generate an episode using the epsilon-greedy policy. We define a function called generate_episode which takes the Q value as an input and returns the episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(Q):\n",
    "    # initialize a list for storing the episode\n",
    "    episode = []\n",
    "\n",
    "    # initialize the state using the reset function\n",
    "    state = env.reset()\n",
    "\n",
    "    # then for each time step\n",
    "    for t in range(num_timesteps):\n",
    "        # select the action according to the epsilon-greedy policy\n",
    "        action = epsilon_greedy_policy_(state, Q)\n",
    "\n",
    "        # perform the selected action and store the next state information\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "        # store the state, action, reward in the episode list\n",
    "        episode.append((state, action, reward))\n",
    "\n",
    "        # if the next state is a final state then break the loop else update the next state to the current\n",
    "        # state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the optimal policy\n",
    "\n",
    "Now, let's learn how to compute the optimal policy. First, let's set the number of iterations, that is, the number of episodes, we want to generate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 50000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned that in the on-policy control method, we will not be given any policy as an input. So, we initialize a random policy in the first iteration and improve the policy iteratively by computing Q value. Since we extract the policy from the Q function, we don't have to explicitly define the policy. As the Q value improves the policy also improves implicitly. That is, in the first iteration we generate episode by extracting the policy (epsilon-greedy) from the initialized Q function. Over a series of iterations, we will find the optimal Q function and hence we also find the optimal policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "max(list(range(env.action_space.n)), key=lambda x: Q[(tuple(state[0]), x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# for each iteration\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[1;32m      3\u001b[0m     \u001b[39m# so, here we pass our initialized Q function to generate an episode\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     episode \u001b[39m=\u001b[39m generate_episode(Q)\n\u001b[1;32m      6\u001b[0m     \u001b[39m# get all the state-action pairs in the episode\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[39m#all_state_action_pairs = [(s, a) for (s, a, r) in episode]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     all_state_action_pairs \u001b[39m=\u001b[39m [(\u001b[39mtuple\u001b[39m(s\u001b[39m.\u001b[39mtolist()), a) \u001b[39mfor\u001b[39;00m (s, a, r) \u001b[39min\u001b[39;00m episode]\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mgenerate_episode\u001b[0;34m(Q)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# then for each time step\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_timesteps):\n\u001b[1;32m     10\u001b[0m     \u001b[39m# select the action according to the epsilon-greedy policy\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     action \u001b[39m=\u001b[39m epsilon_greedy_policy_(state, Q)\n\u001b[1;32m     13\u001b[0m     \u001b[39m# perform the selected action and store the next state information\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     next_state, reward, done, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mepsilon_greedy_policy_\u001b[0;34m(state, Q)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[1;32m      6\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     state_tuple \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(state\u001b[39m.\u001b[39;49mtolist())  \u001b[39m# Convert the NumPy array to a tuple\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn)), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: Q[(state_tuple, x)])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "# for each iteration\n",
    "for i in range(num_iterations):\n",
    "    # so, here we pass our initialized Q function to generate an episode\n",
    "    episode = generate_episode(Q)\n",
    "\n",
    "    # get all the state-action pairs in the episode\n",
    "    # all_state_action_pairs = [(s, a) for (s, a, r) in episode]\n",
    "    all_state_action_pairs = [(tuple(s.tolist()), a) for (s, a, r) in episode]\n",
    "\n",
    "    # store all the rewards obtained in the episode in the rewards list\n",
    "    rewards = [r for (s, a, r) in episode]\n",
    "\n",
    "    # for each step in the episode\n",
    "    for t, (state, action, reward) in enumerate(episode):\n",
    "        # if the state-action pair is occurring for the first time in the episode\n",
    "        if not (state, action) in all_state_action_pairs[0:t]:\n",
    "            # compute the return R of the state-action pair as the sum of rewards\n",
    "            R = sum(rewards[t:])\n",
    "\n",
    "            # update total return of the state-action pair\n",
    "            # total_return[(state, action)] = total_return[(state, action)] + R\n",
    "            total_return[(tuple(state), action)] = (\n",
    "                total_return[(tuple(state), action)] + R\n",
    "            )\n",
    "\n",
    "            # update the number of times the state-action pair is visited\n",
    "            # N[(state, action)] += 1\n",
    "            N[(tuple(state), action)] += 1\n",
    "\n",
    "            # compute the Q value by just taking the average\n",
    "            # Q[(state, action)] = total_return[(state, action)] / N[(state, action)]\n",
    "            Q[(tuple(state), action)] = (\n",
    "                total_return[(tuple(state), action)] / N[(tuple(state), action)]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Q.items(), columns=[\"state_action pair\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
