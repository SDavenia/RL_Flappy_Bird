{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import flappy_bird_gymnasium\n",
    "\n",
    "env_name = \"FlappyBird-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "while True:\n",
    "    # Next action:\n",
    "    # (feed the observation to your agent here)\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Processing:\n",
    "    obs, reward, terminated, _, info = env.step(action)\n",
    "\n",
    "    # Rendering the game:\n",
    "    # (remove this two lines during training)\n",
    "    env.render()\n",
    "    time.sleep(1 / 30)  # FPS\n",
    "\n",
    "    # Checking if the player is still alive\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Control Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCControl:\n",
    "    \"\"\"Implements Monte Carlo Control.\"\"\"\n",
    "\n",
    "    def __init__(self, env, num_states, num_actions, epsilon, gamma):\n",
    "        \"\"\"Parameters\n",
    "        ----------\n",
    "        env:         gym.core.Environment, open gym environment object\n",
    "        num_states:  integer, number of states in the environment\n",
    "        num_actions: integer, number of possible actions\n",
    "        epsilon:     float, the epsilon parameter used for exploration\n",
    "        gamma:       float, discount factor\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def run_mc_control(self, num_episodes, verbose=True):\n",
    "        \"\"\"Performs Monte Carlo control task.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_episodes: integer, number of episodes to run to train RL agent\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        self.Q:              nested dictionary {state: {action: q value}}, final action value function\n",
    "        self.policy:         list of integers of length self.num_states, final policy\n",
    "        rewards_per_episode: numpy array of rewards collected at each episode\n",
    "        \"\"\"\n",
    "        self.init_agent()\n",
    "\n",
    "        rewards_per_episode = np.array([None] * num_episodes)\n",
    "        episode_len = np.array([None] * num_episodes)\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state_action_reward = self.generate_episode(self.policy)\n",
    "            G = self.calculate_returns(state_action_reward)\n",
    "            self.evaluate_policy(G)\n",
    "            self.improve_policy()\n",
    "\n",
    "            # Logging rewards and episode length\n",
    "            total_return = 0\n",
    "            for _, _, reward in state_action_reward:\n",
    "                total_return += reward\n",
    "            rewards_per_episode[episode] = total_return\n",
    "            episode_len = len(state_action_reward)\n",
    "\n",
    "        # Once training is finished, calculate final policy using argmax approach\n",
    "        final_policy = self.argmax(self.Q, self.policy)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Finished training RL agent for {num_episodes} episodes!\")\n",
    "\n",
    "        return self.Q, final_policy, rewards_per_episode, episode_len\n",
    "\n",
    "    def init_agent(self):\n",
    "        \"\"\"Initializes RL agent components:\n",
    "        self.policy:      list of integers of length self.num_states, the action to take at a given state\n",
    "        self.Q:           nested dictionary {state: {action: q value}}, action value function\n",
    "        self.visit_count: nested dictionary {state: {action: count}}, keeps track of how many episodes\n",
    "                          state and action pair were visited for a first time in every episode\n",
    "        \"\"\"\n",
    "        # --------------------------\n",
    "        # Randomly initialize policy, use numpy random.choice method:\n",
    "        # your code here (1 line)\n",
    "        self.policy = np.random.choice(num_actions, num_states)\n",
    "        # --------------------------\n",
    "\n",
    "        self.Q = {}\n",
    "        self.visit_count = {}\n",
    "\n",
    "        for state in range(self.num_states):\n",
    "            self.Q[state] = {}\n",
    "            self.visit_count[state] = {}\n",
    "            for action in range(self.num_actions):\n",
    "                # --------------------------\n",
    "                # Initalize action value (self.Q) and visit count (self.visit_count) dictionaries to zero:\n",
    "                # your code here (~ 2 lines)\n",
    "                self.Q[state][action] = 0\n",
    "                self.visit_count[state][action] = 0\n",
    "                # --------------------------\n",
    "\n",
    "    def generate_episode(self, policy):\n",
    "        \"\"\"Generates episode given current policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        policy: list of integers of length self.num_states, the action to take at a given state\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        state_action_reward: list of tuple (state, action, reward)\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "        s = env.reset()\n",
    "        a = policy[s]\n",
    "\n",
    "        state_action_reward = [(s, a, 0)]\n",
    "        while True:\n",
    "            s, r, terminated, _ = env.step(a)\n",
    "            if terminated:\n",
    "                state_action_reward.append((s, None, r))\n",
    "                break\n",
    "            else:\n",
    "                a = policy[s]\n",
    "                state_action_reward.append((s, a, r))\n",
    "\n",
    "        return state_action_reward\n",
    "\n",
    "    def calculate_returns(self, state_action_reward):\n",
    "        \"\"\"Calculates and returns total discounted reward for each pair of (s, a) appearing in the episode.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_action_reward: list of tuple (state, action, reward)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        G: nested dictionary {state: {action: count}}, contains returns for every pair of (s, a) appearing in the episode\n",
    "\n",
    "        \"\"\"\n",
    "        G = {}\n",
    "        t = 0\n",
    "        for state, action, reward in state_action_reward:\n",
    "            if state not in G:\n",
    "                G[state] = {action: 0}\n",
    "            else:\n",
    "                if action not in G[state]:\n",
    "                    G[state][action] = 0\n",
    "            for s in G.keys():\n",
    "                for a in G[s].keys():\n",
    "                    G[s][a] += reward * gamma**t\n",
    "            t += 1\n",
    "\n",
    "        return G\n",
    "\n",
    "    def evaluate_policy(self, G):\n",
    "        \"\"\"Evaluates current policy using incremental mean and updates action value function self.Q.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G: float, episode return (total discounted reward)\n",
    "        state_action_reward: list of tuple (state, action, reward)\n",
    "        \"\"\"\n",
    "\n",
    "        for state in G.keys():\n",
    "            for action in G[state].keys():\n",
    "                if action:\n",
    "                    # your code here (2 lines): increment self.visit_count and update self.Q for state and action pair\n",
    "                    self.visit_count[state][action] += 1\n",
    "                    self.Q[state][action] += (\n",
    "                        G[state][action] - self.Q[state][action]\n",
    "                    ) / self.visit_count[state][action]\n",
    "                    # --------------------------\n",
    "\n",
    "    def improve_policy(self):\n",
    "        \"\"\"Improves and updates current policy self.policy using epsilon greedy approach.\"\"\"\n",
    "        # Your code here (~3 lines):\n",
    "        # first use argmax method to choose actions greedily: self.policy = argmax\n",
    "        # then replace greedy policy by epsilon greedy approach: self.policy[state] for every state in S = ?\n",
    "        self.policy = self.argmax(self.Q, self.policy)\n",
    "        for state in range(self.num_states):\n",
    "            self.policy[state] = self.get_epsilon_greedy_action(self.policy[state])\n",
    "        # --------------------------\n",
    "\n",
    "    def argmax(self, Q, policy):\n",
    "        \"\"\"\n",
    "        Finds and returns greedy policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Q: nested dictionary {state: {action: q value}}, action value function\n",
    "        policy: list of integers of length self.num_states containing last actions per state\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        next_policy: list of integers of length self.num_states containing next actions with a highest value per state\n",
    "\n",
    "        \"\"\"\n",
    "        next_policy = policy\n",
    "\n",
    "        for state in range(self.num_states):\n",
    "            best_action = None\n",
    "            best_value = float(\"-inf\")\n",
    "            # --------------------------\n",
    "            # Find greedy action to take in every state and assign to policy[state]:\n",
    "            # your code here (~ 5 lines)\n",
    "            for action, value in Q[state].items():\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "            next_policy[state] = best_action\n",
    "            # --------------------------\n",
    "\n",
    "        return next_policy\n",
    "\n",
    "    def get_epsilon_greedy_action(self, greedy_action):\n",
    "        \"\"\"Returns next action using epsilon greedy approach.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        greedy_action: integer, greedy action (action with a maximum Q value)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        next_action: integer, either greedy or random action\n",
    "        \"\"\"\n",
    "        prob = np.random.random()\n",
    "\n",
    "        if prob < 1 - self.epsilon:\n",
    "            # your code here (1 line)\n",
    "            # return ?\n",
    "            return greedy_action\n",
    "            # --------------------------\n",
    "\n",
    "        # your code here (1 line)\n",
    "        # return ?\n",
    "        return np.random.randint(0, self.num_actions)\n",
    "        # --------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "epsilon = 0.4\n",
    "gamma = 0.9\n",
    "n_episodes = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(env_name)\n\u001b[0;32m----> 3\u001b[0m num_states \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mn\n\u001b[1;32m      4\u001b[0m num_actions \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[1;32m      7\u001b[0m mc_model \u001b[39m=\u001b[39m MCControl(env, num_states, num_actions, epsilon, gamma)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "\n",
    "mc_model = MCControl(env, num_states, num_actions, epsilon, gamma)\n",
    "\n",
    "policy = np.array([1, 1, 1, 1, 0, 0, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3])\n",
    "res = mc_model.generate_episode(policy)\n",
    "\n",
    "assert res == [\n",
    "    (0, 1, 0),\n",
    "    (4, 0, 0.0),\n",
    "    (4, 0, 0.0),\n",
    "    (8, 3, 0.0),\n",
    "    (8, 3, 0.0),\n",
    "    (4, 0, 0.0),\n",
    "    (0, 1, 0.0),\n",
    "    (4, 0, 0.0),\n",
    "    (4, 0, 0.0),\n",
    "    (4, 0, 0.0),\n",
    "    (8, 3, 0.0),\n",
    "    (4, 0, 0.0),\n",
    "    (0, 1, 0.0),\n",
    "    (4, 0, 0.0),\n",
    "    (8, 3, 0.0),\n",
    "    (8, 3, 0.0),\n",
    "    (8, 3, 0.0),\n",
    "    (8, 3, 0.0),\n",
    "    (4, 0, 0.0),\n",
    "    (8, 3, 0.0),\n",
    "    (8, 3, 0.0),\n",
    "    (8, 3, 0.0),\n",
    "    (9, 3, 0.0),\n",
    "    (5, None, 0.0),\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
